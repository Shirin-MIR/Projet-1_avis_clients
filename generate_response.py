# -*- coding: utf-8 -*-
"""generate_response.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sdH65eRgChmRjukGN2FvNezTVlQ8JLDT
"""

import os

# Détecte si on veut éviter de charger le vrai modèle (mode tests CI)
SKIP_MODEL = os.environ.get(
    "SKIP_MODEL_DOWNLOAD",
    "false"
).lower() in ("1", "true", "yes")


if not SKIP_MODEL:
 
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        pipeline
    )

    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-3B-Instruct"
    )

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B-Instruct",
        torch_dtype="auto",
        device_map="auto"
    )

    gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=150,
        temperature=0.4,
    )


# ============================
# Fonction API
# ============================
def generer_reponse(texte, sentiment="negative"):
    """
    Génére une réponse polie au client.
    En mode CI, renvoie une réponse simulée.
    """

    # MODE CI = TESTS → On simule
    if SKIP_MODEL:
        return (
            f"[MOCK] Réponse simulée. "
            f"Texte='{texte[:40]}...' "
            f"(sentiment={sentiment})"
        )

    # PROMPT pour Qwen (meilleur style)
    prompt = (
        "Tu es un assistant service client professionnel.\n"
        f"Message du client : {texte}\n"
        f"Sentiment détecté : {sentiment}\n"
        "Rédige une réponse polie, courte et empathique.\n\n"
        "Réponse :"
    )

    sortie = gen_pipe(prompt)[0]["generated_text"]

    # Extraire seulement la réponse après “Réponse :”
    if "Réponse :" in sortie:
        sortie = sortie.split("Réponse :", 1)[1].strip()

    return sortie
